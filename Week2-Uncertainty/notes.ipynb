{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2729eee2",
   "metadata": {},
   "source": [
    "# Uncertainty & Probability\n",
    "\n",
    "## ðŸŽ¯ Introduction\n",
    "\n",
    "**Problem**: AI often has only **partial knowledge** of the world, creating uncertainty.\n",
    "\n",
    "**Goal**: Make optimal decisions given limited information.\n",
    "\n",
    "**Example**: Predicting weather\n",
    "- Know today's weather\n",
    "- Cannot predict tomorrow with 100% accuracy\n",
    "- But can do better than random guessing!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Basic Probability Concepts\n",
    "\n",
    "### Possible Worlds\n",
    "\n",
    "**Definition**: Every possible situation is a **world** (Ï‰)\n",
    "\n",
    "**Example**: Rolling a die has 6 possible worlds\n",
    "- World where die = 1\n",
    "- World where die = 2\n",
    "- ... World where die = 6\n",
    "\n",
    "**Notation**: $P(\\omega)$ = probability of world Ï‰\n",
    "\n",
    "---\n",
    "\n",
    "### Axioms of Probability\n",
    "\n",
    "**Axiom 1**: $0 \\leq P(\\omega) \\leq 1$\n",
    "- $P(\\omega) = 0$ â†’ Impossible event (rolling 7 on standard die)\n",
    "- $P(\\omega) = 1$ â†’ Certain event (rolling < 10 on standard die)\n",
    "- Higher value â†’ More likely to occur\n",
    "\n",
    "**Axiom 2**: Sum of all probabilities = 1\n",
    "$$\\sum_{\\omega} P(\\omega) = 1$$\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Rolling Two Dice\n",
    "\n",
    "**Single die**: $P(R) = \\frac{1}{6}$ (6 equally likely outcomes)\n",
    "\n",
    "**Two dice**: 36 possible outcomes (all equally likely)\n",
    "\n",
    "**Sum probabilities**:\n",
    "- $P(\\text{sum} = 12) = \\frac{1}{36}$ (only 6+6 works)\n",
    "- $P(\\text{sum} = 7) = \\frac{6}{36} = \\frac{1}{6}$ (six ways: 1+6, 2+5, 3+4, 4+3, 5+2, 6+1)\n",
    "\n",
    "**General formula**:\n",
    "$$P(\\text{event}) = \\frac{\\text{number of worlds where event occurs}}{\\text{total number of worlds}}$$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”€ Types of Probability\n",
    "\n",
    "### Unconditional Probability\n",
    "\n",
    "**Definition**: Degree of belief in a proposition **without any evidence**\n",
    "\n",
    "**Examples**:\n",
    "- $P(\\text{rain tomorrow})$\n",
    "- $P(\\text{die} = 6)$\n",
    "\n",
    "**Key**: Result is not dependent on previous events\n",
    "\n",
    "---\n",
    "\n",
    "### Conditional Probability\n",
    "\n",
    "**Definition**: Degree of belief given some evidence already revealed\n",
    "\n",
    "**Notation**: $P(a \\mid b)$ reads as \"probability of $a$ given $b$\"\n",
    "\n",
    "**Examples**:\n",
    "- $P(\\text{rain today} \\mid \\text{rain yesterday})$\n",
    "- $P(\\text{disease} \\mid \\text{positive test})$\n",
    "- $P(\\text{sum = 12} \\mid \\text{first die = 6})$\n",
    "\n",
    "---\n",
    "\n",
    "### Conditional Probability Formula\n",
    "\n",
    "$$P(a \\mid b) = \\frac{P(a \\land b)}{P(b)}$$\n",
    "\n",
    "**In words**: Probability of $a$ given $b$ = (Probability of both $a$ and $b$) / (Probability of $b$)\n",
    "\n",
    "**Intuition**: We restrict to worlds where $b$ is true, then find what fraction also have $a$ true.\n",
    "\n",
    "---\n",
    "\n",
    "### Equivalent Forms\n",
    "\n",
    "$$P(a \\land b) = P(a \\mid b) \\cdot P(b)$$\n",
    "\n",
    "$$P(a \\land b) = P(b \\mid a) \\cdot P(a)$$\n",
    "\n",
    "**Key insight**: $P(a \\land b) = P(b \\land a)$ (order doesn't matter)\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Conditional Probability with Dice\n",
    "\n",
    "**Query**: $P(\\text{sum = 12} \\mid \\text{first die = 6})$\n",
    "\n",
    "**Step 1**: Restrict worlds to where first die = 6\n",
    "\n",
    "**Possible outcomes**: (6,1), (6,2), (6,3), (6,4), (6,5), (6,6)\n",
    "\n",
    "**Step 2**: Count how many have sum = 12\n",
    "\n",
    "Only (6,6) works â†’ 1 out of 6 outcomes\n",
    "\n",
    "**Answer**: $P(\\text{sum = 12} \\mid \\text{first die = 6}) = \\frac{1}{6}$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ² Random Variables\n",
    "\n",
    "**Definition**: A variable with a domain of possible values it can take\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "**Roll** (die outcome):\n",
    "- Domain: $\\{1, 2, 3, 4, 5, 6\\}$\n",
    "\n",
    "**Flight** (status):\n",
    "- Domain: $\\{\\text{on time, delayed, canceled}\\}$\n",
    "\n",
    "---\n",
    "\n",
    "### Probability Distribution\n",
    "\n",
    "**Example**:\n",
    "- $P(\\text{Flight} = \\text{on time}) = 0.6$\n",
    "- $P(\\text{Flight} = \\text{delayed}) = 0.3$\n",
    "- $P(\\text{Flight} = \\text{canceled}) = 0.1$\n",
    "\n",
    "**Vector notation**: $P(\\text{Flight}) = \\langle 0.6, 0.3, 0.1 \\rangle$\n",
    "\n",
    "**Must sum to 1**: $0.6 + 0.3 + 0.1 = 1$ âœ“\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— Independence\n",
    "\n",
    "**Definition**: Occurrence of one event does NOT affect probability of the other\n",
    "\n",
    "**Mathematical definition**:\n",
    "$$P(a \\land b) = P(a) \\cdot P(b)$$\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "**Independent**:\n",
    "- Two die rolls\n",
    "- Coin flips\n",
    "\n",
    "**Dependent**:\n",
    "- Clouds in morning and rain in afternoon\n",
    "- Studying and passing exam\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ Bayes' Rule\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$P(b \\mid a) = \\frac{P(a \\mid b) \\cdot P(b)}{P(a)}$$\n",
    "\n",
    "**Key insight**: Allows us to \"flip\" conditional probabilities!\n",
    "- Know: $P(\\text{visible effect} \\mid \\text{unknown cause})$\n",
    "- Want: $P(\\text{unknown cause} \\mid \\text{visible effect})$\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Rain Prediction\n",
    "\n",
    "**Given**:\n",
    "- 80% of rainy afternoons start with cloudy mornings: $P(\\text{clouds} \\mid \\text{rain}) = 0.8$\n",
    "- 40% of days have cloudy mornings: $P(\\text{clouds}) = 0.4$\n",
    "- 10% of days have rainy afternoons: $P(\\text{rain}) = 0.1$\n",
    "\n",
    "**Find**: $P(\\text{rain} \\mid \\text{clouds})$\n",
    "\n",
    "**Solution**:\n",
    "$$P(\\text{rain} \\mid \\text{clouds}) = \\frac{P(\\text{clouds} \\mid \\text{rain}) \\cdot P(\\text{rain})}{P(\\text{clouds})} = \\frac{0.8 \\times 0.1}{0.4} = \\frac{0.08}{0.4} = 0.2$$\n",
    "\n",
    "**Answer**: 20% chance of rain given cloudy morning\n",
    "\n",
    "---\n",
    "\n",
    "### Medical Diagnosis Application\n",
    "\n",
    "**Known**: $P(\\text{test positive} \\mid \\text{disease})$ from medical trials\n",
    "\n",
    "**Want**: $P(\\text{disease} \\mid \\text{test positive})$ for diagnosis\n",
    "\n",
    "Bayes' rule lets us calculate this!\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Joint Probability\n",
    "\n",
    "**Definition**: Likelihood of **multiple events all occurring**\n",
    "\n",
    "ðŸ“¸ **[IMAGE 3: Joint probability tables for clouds and rain]**\n",
    "![Alt text](./01-joint.png)\n",
    "\n",
    "### Example: Clouds and Rain\n",
    "\n",
    "**Individual probabilities**:\n",
    "- $P(C = \\text{cloud}) = 0.4$, $P(C = \\neg\\text{cloud}) = 0.6$\n",
    "- $P(R = \\text{rain}) = 0.1$, $P(R = \\neg\\text{rain}) = 0.9$\n",
    "\n",
    "**Joint probability table**:\n",
    "\n",
    "|   | $R = \\text{rain}$ | $R = \\neg\\text{rain}$ |\n",
    "|---|------|---------|\n",
    "| $C = \\text{cloud}$ | 0.08 | 0.32 |\n",
    "| $C = \\neg\\text{cloud}$ | 0.02 | 0.58 |\n",
    "\n",
    "**Interpretation**:\n",
    "- $P(C \\land R) = 0.08$ â†’ 8% chance of both clouds and rain\n",
    "- $P(\\neg C \\land \\neg R) = 0.58$ â†’ 58% chance of neither\n",
    "\n",
    "---\n",
    "\n",
    "### Deducing Conditional from Joint\n",
    "\n",
    "**Formula**: \n",
    "$$P(C \\mid \\text{rain}) = \\frac{P(C, \\text{rain})}{P(\\text{rain})}$$\n",
    "\n",
    "**Calculation**:\n",
    "$$P(C \\mid \\text{rain}) = \\alpha \\langle 0.08, 0.02 \\rangle$$\n",
    "\n",
    "Where $\\alpha$ normalizes so probabilities sum to 1:\n",
    "- $\\alpha \\cdot 0.08 + \\alpha \\cdot 0.02 = 1$\n",
    "- $\\alpha \\cdot 0.10 = 1$\n",
    "- $\\alpha = 10$\n",
    "\n",
    "**Result**: $P(C \\mid \\text{rain}) = \\langle 0.8, 0.2 \\rangle$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Probability Rules\n",
    "\n",
    "### 1. Negation\n",
    "$$P(\\neg a) = 1 - P(a)$$\n",
    "\n",
    "**Why**: All possible worlds sum to 1\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Inclusion-Exclusion\n",
    "$$P(a \\lor b) = P(a) + P(b) - P(a \\land b)$$\n",
    "\n",
    "**Example**: Ice cream and cookies\n",
    "- Eat ice cream: 80% of days\n",
    "- Eat cookies: 70% of days\n",
    "- $P(\\text{ice cream} \\lor \\text{cookies}) = 0.8 + 0.7 - P(\\text{both}) \\neq 1.5$\n",
    "- Must subtract overlap to avoid double-counting!\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Marginalization\n",
    "$$P(a) = P(a, b) + P(a, \\neg b)$$\n",
    "\n",
    "**General form**:\n",
    "$$P(X = x_i) = \\sum_{j} P(X = x_i, Y = y_j)$$\n",
    "\n",
    "**Example**:\n",
    "$$P(C = \\text{cloud}) = P(C = \\text{cloud}, R = \\text{rain}) + P(C = \\text{cloud}, R = \\neg\\text{rain})$$\n",
    "$$= 0.08 + 0.32 = 0.4$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Conditioning\n",
    "$$P(a) = P(a \\mid b) \\cdot P(b) + P(a \\mid \\neg b) \\cdot P(\\neg b)$$\n",
    "\n",
    "**General form**:\n",
    "$$P(X = x_i) = \\sum_{j} P(X = x_i \\mid Y = y_j) \\cdot P(Y = y_j)$$\n",
    "\n",
    "---\n",
    "\n",
    "# Bayesian Networks\n",
    "\n",
    "## ðŸ•¸ï¸ What is a Bayesian Network?\n",
    "\n",
    "**Definition**: Data structure representing dependencies among random variables\n",
    "\n",
    "**Properties**:\n",
    "- Directed graph (arrows show dependencies)\n",
    "- Each node = random variable\n",
    "- Arrow from $X$ to $Y$ means $X$ is a **parent** of $Y$\n",
    "- Each node has: $P(X \\mid \\text{Parents}(X))$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Š Example: Getting to Appointment\n",
    "\n",
    "ðŸ“¸ **[IMAGE 2: Bayesian Network with Rain â†’ Maintenance, Train â†’ Appointment]**\n",
    "![Alt text](./02-computation-joint.png)\n",
    "\n",
    "### Variables:\n",
    "1. **Rain**: $\\{\\text{none, light, heavy}\\}$\n",
    "2. **Maintenance**: $\\{\\text{yes, no}\\}$\n",
    "3. **Train**: $\\{\\text{on time, delayed}\\}$\n",
    "4. **Appointment**: $\\{\\text{attend, miss}\\}$\n",
    "\n",
    "---\n",
    "\n",
    "### Probability Tables\n",
    "\n",
    "**Rain** (root node - no parents):\n",
    "\n",
    "| none | light | heavy |\n",
    "|------|-------|-------|\n",
    "| 0.7  | 0.2   | 0.1   |\n",
    "\n",
    "---\n",
    "\n",
    "**Maintenance** (parent: Rain):\n",
    "\n",
    "| R | yes | no |\n",
    "|---|-----|-----|\n",
    "| none | 0.4 | 0.6 |\n",
    "| light | 0.2 | 0.8 |\n",
    "| heavy | 0.1 | 0.9 |\n",
    "\n",
    "---\n",
    "\n",
    "**Train** (parents: Rain, Maintenance):\n",
    "\n",
    "| R | M | on time | delayed |\n",
    "|---|---|---------|---------|\n",
    "| none | yes | 0.8 | 0.2 |\n",
    "| none | no | 0.9 | 0.1 |\n",
    "| light | yes | 0.6 | 0.4 |\n",
    "| light | no | 0.7 | 0.3 |\n",
    "| heavy | yes | 0.4 | 0.6 |\n",
    "| heavy | no | 0.5 | 0.5 |\n",
    "\n",
    "---\n",
    "\n",
    "**Appointment** (parent: Train):\n",
    "\n",
    "| T | attend | miss |\n",
    "|---|--------|------|\n",
    "| on time | 0.9 | 0.1 |\n",
    "| delayed | 0.6 | 0.4 |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”‘ Key Insight: Conditional Independence\n",
    "\n",
    "**Important**: Appointment only depends on Train, not on Rain or Maintenance!\n",
    "\n",
    "**Why**: Once we know the train status, rain and maintenance don't give additional information.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§® Computing Joint Probabilities\n",
    "\n",
    "### General Formula for Bayesian Networks\n",
    "\n",
    "$$P(X_1, X_2, \\ldots, X_n) = \\prod_{i=1}^{n} P(X_i \\mid \\text{Parents}(X_i))$$\n",
    "\n",
    "**For our network**:\n",
    "$$P(\\text{Rain, Maintenance, Train, Appointment}) = $$\n",
    "$$P(\\text{Rain}) \\cdot P(\\text{Maintenance} \\mid \\text{Rain}) \\cdot P(\\text{Train} \\mid \\text{Rain, Maintenance}) \\cdot P(\\text{Appointment} \\mid \\text{Train})$$\n",
    "\n",
    "---\n",
    "\n",
    "### Example: $P(\\text{light, no, delayed, miss})$\n",
    "\n",
    "**Question**: What's the probability of light rain, no maintenance, delayed train, and missing appointment?\n",
    "\n",
    "**Solution** (using chain rule with conditional independence):\n",
    "\n",
    "$$P(\\text{light, no, delayed, miss}) = $$\n",
    "$$P(\\text{light}) \\cdot P(\\text{no} \\mid \\text{light}) \\cdot P(\\text{delayed} \\mid \\text{light, no}) \\cdot P(\\text{miss} \\mid \\text{delayed})$$\n",
    "\n",
    "**From tables**:\n",
    "- $P(\\text{light}) = 0.2$\n",
    "- $P(\\text{no} \\mid \\text{light}) = 0.8$\n",
    "- $P(\\text{delayed} \\mid \\text{light, no}) = 0.3$\n",
    "- $P(\\text{miss} \\mid \\text{delayed}) = 0.4$\n",
    "\n",
    "**Final calculation**:\n",
    "$$P(\\text{light, no, delayed, miss}) = 0.2 \\times 0.8 \\times 0.3 \\times 0.4 = 0.0192$$\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Works: The Derivation\n",
    "\n",
    "**Naive chain rule** (without using the network):\n",
    "$$P(A, B, C, D) = P(A) \\cdot P(B \\mid A) \\cdot P(C \\mid A, B) \\cdot P(D \\mid A, B, C)$$\n",
    "\n",
    "**With Bayesian network** (using conditional independence):\n",
    "1. Rain has no parents â†’ $P(\\text{Rain})$\n",
    "2. Maintenance depends only on Rain â†’ $P(\\text{Maintenance} \\mid \\text{Rain})$\n",
    "3. Train depends on Rain AND Maintenance â†’ $P(\\text{Train} \\mid \\text{Rain, Maintenance})$\n",
    "4. Appointment depends only on Train â†’ $P(\\text{Appointment} \\mid \\text{Train})$\n",
    "\n",
    "**Result**: Last term simplifies from $P(D \\mid A, B, C)$ to just $P(D \\mid C)$!\n",
    "\n",
    "This dramatically reduces storage and computation! ðŸŽ‰\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Inference in Bayesian Networks\n",
    "\n",
    "### Components\n",
    "\n",
    "**Query variable X**: What we want to compute probability distribution for\n",
    "\n",
    "**Evidence variables E**: Variables we've observed (value = e)\n",
    "\n",
    "**Hidden variables Y**: Variables not queried and not observed\n",
    "\n",
    "**Goal**: Calculate $P(X \\mid e)$\n",
    "\n",
    "---\n",
    "\n",
    "### Example Query\n",
    "\n",
    "**Find**: $P(\\text{Appointment} \\mid \\text{light, no})$\n",
    "\n",
    "**Given**: Light rain, no maintenance\n",
    "\n",
    "**Unknown**: Train status (hidden variable)\n",
    "\n",
    "---\n",
    "\n",
    "### Inference by Enumeration\n",
    "\n",
    "ðŸ“¸ **[IMAGE 1: Inference by Enumeration formula]**\n",
    "![Alt text](./03-inference.png)\n",
    "\n",
    "\n",
    "**Formula**:\n",
    "$$P(X \\mid e) = \\alpha P(X, e) = \\alpha \\sum_{y} P(X, e, y)$$\n",
    "\n",
    "Where:\n",
    "- $X$ = query variable\n",
    "- $e$ = evidence\n",
    "- $y$ = each possible value of hidden variables\n",
    "- $\\alpha$ = normalization constant\n",
    "\n",
    "---\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "**Query**: $P(\\text{Appointment} \\mid \\text{light, no})$\n",
    "\n",
    "**Step 1**: Express as proportion\n",
    "$$P(\\text{Appointment} \\mid \\text{light, no}) = \\alpha P(\\text{Appointment, light, no})$$\n",
    "\n",
    "**Step 2**: Marginalize over hidden variable (Train)\n",
    "$$= \\alpha [P(\\text{Appointment, light, no, delayed}) + P(\\text{Appointment, light, no, on time})]$$\n",
    "\n",
    "**Step 3**: Expand using chain rule for each term\n",
    "\n",
    "**Step 4**: Normalize so probabilities sum to 1\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ² Approximate Inference: Sampling\n",
    "\n",
    "### Why Sampling?\n",
    "\n",
    "**Problem**: Inference by enumeration is slow for large networks\n",
    "\n",
    "**Solution**: Use **approximate inference** through sampling\n",
    "- Lose some precision\n",
    "- Gain massive scalability\n",
    "\n",
    "---\n",
    "\n",
    "### How Sampling Works\n",
    "\n",
    "**Basic idea**: Generate random samples according to probability distributions\n",
    "\n",
    "**Example: Rolling a die**\n",
    "1. Roll die 600 times, record results\n",
    "2. Count occurrences: 1â†’100, 2â†’98, 3â†’105, etc.\n",
    "3. Divide by total: $\\frac{100}{600} \\approx 0.167 \\approx \\frac{1}{6}$\n",
    "\n",
    "**Result**: Approximate distribution close to true distribution\n",
    "\n",
    "---\n",
    "\n",
    "### Sampling in Bayesian Networks\n",
    "\n",
    "**Process**:\n",
    "1. Sample Rain â†’ get \"none\" (with probability 0.7)\n",
    "2. Sample Maintenance given Rain = none â†’ get \"no\" (with probability 0.6)\n",
    "3. Sample Train given Rain = none, Maintenance = no â†’ get \"on time\"\n",
    "4. Sample Appointment given Train = on time â†’ get \"attend\"\n",
    "\n",
    "**One sample**: (none, no, on time, attend)\n",
    "\n",
    "**Repeat 10,000 times** â†’ Get approximate probability distribution!\n",
    "\n",
    "---\n",
    "\n",
    "### Answering Queries with Samples\n",
    "\n",
    "**Unconditional**: $P(\\text{Train = on time})$\n",
    "$$P(\\text{Train = on time}) \\approx \\frac{\\text{count(Train = on time)}}{\\text{total samples}}$$\n",
    "\n",
    "**Conditional**: $P(\\text{Rain = light} \\mid \\text{Train = on time})$\n",
    "1. Filter: Keep only samples where Train = on time\n",
    "2. Count: How many have Rain = light?\n",
    "3. Divide: By number of filtered samples\n",
    "\n",
    "---\n",
    "\n",
    "### Likelihood Weighting (Improved Sampling)\n",
    "\n",
    "**Problem**: Rejection sampling discards many samples\n",
    "\n",
    "**Solution**: Fix evidence variables, weight samples\n",
    "\n",
    "**Steps**:\n",
    "1. Fix evidence variables to observed values\n",
    "2. Sample non-evidence variables normally\n",
    "3. Weight each sample by $P(\\text{evidence} \\mid \\text{sampled parents})$\n",
    "\n",
    "**Example**: If evidence is Train = on time\n",
    "- Sample Rain, Maintenance normally\n",
    "- Always set Train = on time\n",
    "- Weight by $P(\\text{Train = on time} \\mid \\text{Rain, Maintenance})$\n",
    "\n",
    "---\n",
    "\n",
    "# Markov Models\n",
    "\n",
    "## â° Introduction: Time Dimension\n",
    "\n",
    "**Problem**: So far, we haven't represented time in our models\n",
    "\n",
    "**Solution**: Markov models introduce time as a dimension\n",
    "\n",
    "**Notation**:\n",
    "- $X_t$ = current state\n",
    "- $X_{t+1}$ = next state\n",
    "- $X_{t+2}$ = state after next, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ The Markov Assumption\n",
    "\n",
    "**Definition**: Current state depends on only a **finite fixed number** of previous states\n",
    "\n",
    "**Example**: Weather prediction\n",
    "- Could use all weather data from past year\n",
    "- Markov assumption: Only use yesterday's weather\n",
    "- Much more manageable computationally!\n",
    "\n",
    "**Simplest case**: Current depends only on immediately previous state\n",
    "\n",
    "---\n",
    "\n",
    "## â›“ï¸ Markov Chain\n",
    "\n",
    "**Definition**: Sequence of random variables where each follows the Markov assumption\n",
    "\n",
    "**Need**: **Transition model** specifying probability of next state given current state\n",
    "\n",
    "### Weather Example\n",
    "\n",
    "**Transition model**:\n",
    "\n",
    "|  | Tomorrow = sun | Tomorrow = rain |\n",
    "|---|---|---|\n",
    "| Today = sun | 0.8 | 0.2 |\n",
    "| Today = rain | 0.3 | 0.7 |\n",
    "\n",
    "**Interpretation**:\n",
    "- Sunny days tend to follow sunny days (0.8)\n",
    "- Rainy days tend to follow rainy days (0.7)\n",
    "\n",
    "---\n",
    "\n",
    "### Example Markov Chain\n",
    "\n",
    "Starting with sun:\n",
    "\n",
    "Sun â†’ Sun (0.8) â†’ Sun (0.8) â†’ Rain (0.2) â†’ Rain (0.7) â†’ Sun (0.3) â†’ ...\n",
    "\n",
    "**Query**: \"What's probability of 4 rainy days in a row?\"\n",
    "\n",
    "Calculate: $P(\\text{rain}) \\times P(\\text{rain} \\mid \\text{rain})^3$\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ‘ï¸ Hidden Markov Models (HMM)\n",
    "\n",
    "### Concept\n",
    "\n",
    "**Problem**: Sometimes we can't observe the actual state, only **emissions** from it\n",
    "\n",
    "**Components**:\n",
    "- **Hidden state**: What we want to infer\n",
    "- **Observation**: What we can measure\n",
    "\n",
    "---\n",
    "\n",
    "### Examples\n",
    "\n",
    "**Robot navigation**:\n",
    "- Hidden state: Robot position\n",
    "- Observation: Sensor readings\n",
    "\n",
    "**Speech recognition**:\n",
    "- Hidden state: Words spoken\n",
    "- Observation: Audio waveforms\n",
    "\n",
    "**Website analytics**:\n",
    "- Hidden state: User engagement level\n",
    "- Observation: Click/scroll data\n",
    "\n",
    "---\n",
    "\n",
    "### Weather Example with HMM\n",
    "\n",
    "**Scenario**: AI is indoors, can't see weather\n",
    "\n",
    "**Hidden state**: Weather (sun/rain)\n",
    "\n",
    "**Observation**: Did people bring umbrellas?\n",
    "\n",
    "**Sensor model** (emission probabilities):\n",
    "\n",
    "|  | Umbrella | No umbrella |\n",
    "|---|---|---|\n",
    "| Sun | 0.2 | 0.8 |\n",
    "| Rain | 0.9 | 0.1 |\n",
    "\n",
    "---\n",
    "\n",
    "### Sensor Markov Assumption\n",
    "\n",
    "**Assumption**: Observation depends **only** on corresponding hidden state\n",
    "\n",
    "**Example**: Umbrella decision depends only on weather, not personality, etc.\n",
    "\n",
    "**Simplification**: Ignore other factors for tractability\n",
    "\n",
    "---\n",
    "\n",
    "### HMM Structure\n",
    "\n",
    "**Two-layer Markov chain**:\n",
    "- **Top layer** ($X$): Hidden states (sun/rain)\n",
    "- **Bottom layer** ($E$): Observations (umbrella/no umbrella)\n",
    "- **Horizontal arrows**: Transitions between states\n",
    "- **Vertical arrows**: Emissions from states\n",
    "\n",
    "```\n",
    "Xâ‚ â†’ Xâ‚‚ â†’ Xâ‚ƒ â†’ Xâ‚„ â†’ ...  (hidden states)\n",
    "â†“    â†“    â†“    â†“\n",
    "Eâ‚   Eâ‚‚   Eâ‚ƒ   Eâ‚„   ...  (observations)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ HMM Tasks\n",
    "\n",
    "### 1. Filtering\n",
    "**Goal**: $P(X_t \\mid e_1, \\ldots, e_t)$\n",
    "\n",
    "**Example**: Given umbrella observations until today, what's probability it's raining today?\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Prediction\n",
    "**Goal**: $P(X_{t+k} \\mid e_1, \\ldots, e_t)$ where $k > 0$\n",
    "\n",
    "**Example**: Given observations until today, what's probability of rain tomorrow?\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Smoothing\n",
    "**Goal**: $P(X_k \\mid e_1, \\ldots, e_t)$ where $k < t$\n",
    "\n",
    "**Example**: Given today's observations, what was probability of rain yesterday?\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Most Likely Explanation\n",
    "**Goal**: Find sequence $x_1, \\ldots, x_t$ that maximizes $P(x_1, \\ldots, x_t \\mid e_1, \\ldots, e_t)$\n",
    "\n",
    "**Example**: Given umbrella sequence, what's most likely weather sequence?\n",
    "\n",
    "**Application**: Speech recognition (audio â†’ most likely words)\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Most Likely Explanation\n",
    "\n",
    "**Observations**: \n",
    "[umbrella, umbrella, no umbrella, umbrella, umbrella, umbrella, umbrella, no umbrella, no umbrella]\n",
    "\n",
    "**HMM prediction** (most likely weather sequence):\n",
    "[rain, rain, sun, rain, rain, rain, rain, sun, sun]\n",
    "\n",
    "**How it works**: Algorithm finds sequence that best explains observations given transition and sensor models\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Key Takeaways\n",
    "\n",
    "### Probability Basics\n",
    "âœ… Probability ranges from 0 to 1, sums to 1\n",
    "âœ… Conditional probability: $P(a \\mid b) = \\frac{P(a \\land b)}{P(b)}$\n",
    "âœ… Bayes' rule flips conditionals\n",
    "âœ… Independence: $P(a \\land b) = P(a) \\cdot P(b)$\n",
    "\n",
    "### Bayesian Networks\n",
    "âœ… Directed graphs showing dependencies\n",
    "âœ… Joint probability from chain rule + conditional independence\n",
    "âœ… Dramatically reduces storage and computation\n",
    "âœ… Inference finds $P(X \\mid e)$ using enumeration or sampling\n",
    "\n",
    "### Markov Models\n",
    "âœ… Markov assumption: Current depends on finite previous states\n",
    "âœ… Markov chains: Sequences with transition models\n",
    "âœ… Hidden Markov models: Hidden states + observations\n",
    "âœ… Four tasks: filtering, prediction, smoothing, most likely explanation\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”® Applications\n",
    "\n",
    "- **Medical diagnosis**: Symptoms â†’ Disease probability\n",
    "- **Weather forecasting**: Past weather â†’ Future predictions\n",
    "- **Speech recognition**: Audio â†’ Words\n",
    "- **Robot localization**: Sensors â†’ Position\n",
    "- **Spam filtering**: Email features â†’ Spam probability\n",
    "- **Stock prediction**: Past prices â†’ Future trends\n",
    "- **Recommendation systems**: User behavior â†’ Preferences"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
